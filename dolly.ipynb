{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rqiu/Developer/chitchat/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, PreTrainedModel, PreTrainedTokenizer\n",
    "# import torch\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 12.2G/12.2G [05:33<00:00, 36.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"databricks/dolly-v1-6b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTJForCausalLM(\n",
       "  (transformer): GPTJModel(\n",
       "    (wte): Embedding(50403, 4096)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-27): 28 x GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=50403, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt and response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_FORMAT = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "def generate_response(instruction: str, *, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, \n",
    "                      do_sample: bool = True, max_new_tokens: int = 256, top_p: float = 0.92, top_k: int = 0, **kwargs) -> str:\n",
    "    input_ids = tokenizer(PROMPT_FORMAT.format(instruction=instruction), return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
    "\n",
    "    # each of these is encoded to a single token\n",
    "    response_key_token_id = tokenizer.encode(\"### Response:\")[0]\n",
    "    end_key_token_id = tokenizer.encode(\"### End\")[0]\n",
    "\n",
    "    gen_tokens = model.generate(input_ids, pad_token_id=tokenizer.pad_token_id, eos_token_id=end_key_token_id,\n",
    "                                do_sample=do_sample, max_new_tokens=max_new_tokens, top_p=top_p, top_k=top_k, **kwargs)[0].cpu()\n",
    "\n",
    "    # find where the response begins\n",
    "    response_positions = np.where(gen_tokens == response_key_token_id)[0]\n",
    "\n",
    "    if len(response_positions) >= 0:\n",
    "        response_pos = response_positions[0]\n",
    "        \n",
    "        # find where the response ends\n",
    "        end_pos = None\n",
    "        end_positions = np.where(gen_tokens == end_key_token_id)[0]\n",
    "        if len(end_positions) > 0:\n",
    "            end_pos = end_positions[0]\n",
    "\n",
    "        return tokenizer.decode(gen_tokens[response_pos + 1 : end_pos]).strip()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You're going to love @Databricks's new language model, Dolly! Get it now and revolutionize text processing tasks! #AI #ML #Databricks\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample similar to: \"Excited to announce the release of Dolly, a powerful new language model from Databricks! #AI #Databricks\"\n",
    "generate_response(\"Write a tweet announcing Dolly, a large language model from Databricks.\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current President of the United States is Donald Trump, and he is 71 years old.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response(\"Who's the president of the United States and how old is he now?\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Welcome to ChatGPT, a conversational AI chatbot for everyday use. ChatGPT is designed to help you connect with friends and family more easily by providing intelligent answers to your questions. ChatGPT uses natural language processing (NLP) to understand the context of your questions and provides relevant answers quickly.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response(\"Write me a paragraph of the introduction to ChatGPT\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In a world where teenagers are connected to each other and to the world in unprecedented ways, the sheer number of social networks available has drastically changed the way teenagers interact and communicate. Social media has allowed teens to stay connected, even when their physical presence is not readily available. They can easily communicate with friends and family, share photos and videos, and connect with other people around the world. However, there are risks associated with the easy access to this level of technology. Teens have been exposed to inappropriate and sometimes dangerous content, they are increasingly spending time on these platforms without parental supervision, and they may be overly reliant on them. Parents should be mindful of the content that their children are exposed to online and help them understand the importance of self-control when using social media.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response(\"Imitate Charles Dickens' writing style of A Tale of Two Cities to write a paragraph about social network's impact on teenagers today.\", model=model, tokenizer=tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
